# vicuna_test

ç›´æ¥å¯åŠ¨æ¨ç†çš„æƒé‡æ–‡ä»¶æ•´ç†åå¯å‘å‡º~åç»­å¾®è°ƒçš„å°è¯•ä¹Ÿåœ¨githubæ›´æ–°ï¼ˆå¦‚æœæœ‰ğŸ˜Šï¼‰ï¼Œæ„Ÿå…´è¶£çš„è€æ¿å¯ä»¥ç‚¹ä¸ªstar

æ–°ç‰ˆæœ¬v1.1
### 1.å®‰è£…fastchatå’Œtransformers
```
pip install fschat
# Install the latest main branch of huggingface/transformers
pip install git+https://github.com/huggingface/transformers
```
### 2.ä¸‹è½½LLaMAæƒé‡
è¯¥é¡¹ç›®ä¹Ÿæ˜¯ç”±Metaçš„LLaMA 7Bå¾®è°ƒè€Œæ¥ï¼Œå› æ­¤éœ€è¦ä¸‹è½½LLaMAæƒé‡ï¼šå¯ä»¥ä»è¿™é‡Œç”³è¯·å¹¶ä¸‹è½½LLaMAï¼Œä¹Ÿå¯ä»¥ï¼š
```
pip install pyllama -U
python -m llama.download --model_size 7B
#python -m llama.download --model_size 13B
```

![](https://raw.githubusercontent.com/noobimp/vicuna_test/main/1.png#pic_center)


7Bä¸‹è½½å®Œæˆåæ˜¯è¿™5ä¸ªæ–‡ä»¶ï¼Œæƒé‡æ–‡ä»¶12.55G

### 3.æƒé‡è½¬æ¢
æ³¨æ„æœ€æ–°åˆ†æ”¯çš„è½¬æ¢è„šæœ¬å¯èƒ½ä¼šæŠ¥é”™ï¼Œæˆ‘ç”¨çš„æ˜¯è¿™ä¸ªï¼š
https://github.com/huggingface/transformers/blob/9eae4aa57650c1dbe1becd4e0979f6ad1e572ac0/src/transformers/models/llama/convert_llama_weights_to_hf.py

å¯ä»¥æŠŠè¿™ä¸ªè„šæœ¬æ”¾åœ¨pyllama_dataç›®å½•ä¸‹ï¼Œç„¶åæ‰§è¡Œï¼š
```
python convert_llama_weights_to_hf.py --input_dir ./ --model_size 7B --output_dir ./output/7B
```
å¾—åˆ°output/7B

![](https://raw.githubusercontent.com/noobimp/vicuna_test/main/2.png#pic_center)


ä¸‹é¢çš„å‘½ä»¤å¯ä»¥è‡ªåŠ¨ä¸‹è½½vicunaçš„deltaæƒé‡å¹¶å’Œè½¬æ¢åçš„LLaMAæƒé‡èåˆï¼Œè¾“å‡ºè‡³targetç›®å½•ä¸‹
```
#baseå’Œtargetè·¯å¾„å¯ä»¥æŒ‰ä½ æœ¬åœ°è°ƒæ•´
python -m fastchat.model.apply_delta \
    --base ./pyllama_data/output/7B \
    --target ./vicuna_data/vicuna-7b  \
    --delta lmsys/vicuna-7b-delta-v1.1
```

æ­¤è½¬æ¢å‘½ä»¤éœ€è¦å¤§çº¦30GBå†…å­˜ï¼Œå¾—åˆ°vicuna_data/vicuna-7bï¼Œæ€»å¤§å°çº¦14GBã€‚

![](https://raw.githubusercontent.com/noobimp/vicuna_test/main/3.png#pic_center)
    
### 4.æ¨ç†
GPU

å¯åŠ¨æ¨ç†Vicuna-13Bå¤§çº¦éœ€è¦28GBæ˜¾å­˜ï¼ŒVicuna-7Béœ€è¦14GBæ˜¾å­˜ï¼Œå¯ä»¥é€šè¿‡--num-gpuså¤šå¡æ¨ç†ã€‚

```
python -m fastchat.serve.cli --model-path ./vicuna_data/vicuna-7b --num-gpus 2
```

CPU

Vicuna-13Bå¤§çº¦éœ€è¦60GBå†…å­˜ï¼ŒVicuna-7Bå¤§çº¦éœ€è¦30GBå†…å­˜ã€‚

```
python -m fastchat.serve.cli --model-path ./vicuna_data/vicuna-7b --device cpu
```

å¯ä½¿ç”¨--load-8bitå‹ç¼©ï¼Œâ€œè¿™å¯ä»¥å°†å†…å­˜ä½¿ç”¨é‡å‡å°‘å¤§çº¦ä¸€åŠï¼ŒåŒæ—¶æ¨¡å‹è´¨é‡ç•¥æœ‰ä¸‹é™ï¼Œ8bitå‹ç¼©çš„ Vicuna-13Bå¯ä»¥åœ¨å•ä¸ª NVIDIA 3090/4080/V100(16GB) GPU ä¸Šè¿è¡Œã€‚â€
è€Œ8bitå‹ç¼©çš„7Båœ¨CPUä¸Šå¯åŠ¨åï¼Œå†…å­˜å ç”¨çº¦5Gï¼Œè¾ƒçŸ­çš„å›ç­”å†…å­˜å ç”¨çº¦8-9Gï¼Œé€Ÿåº¦æ„Ÿäººï¼Œæˆ‘ä¼°è®¡10sæ‰1-2ä¸ªtokenã€‚
    
![](https://raw.githubusercontent.com/noobimp/vicuna_test/main/4.png#pic_center)

å¯¹äº7Bç‰ˆæœ¬ï¼Œè®©å®ƒè®²è®²UCASæ€ä¹ˆæ ·ï¼Œåšä¸€é“ä¸¤æ•°ä¹‹å’Œa+b=targetï¼Œå¤§è‡´æ•ˆæœå¦‚å›¾ã€‚
